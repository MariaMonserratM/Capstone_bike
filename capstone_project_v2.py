# -*- coding: utf-8 -*-
"""Capstone_Project_v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1APqlbZ7JxqSfQjFejhEJxNvwgqIDqgHB
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import re
import ast
import typing
import requests
import dask.dataframe as dd
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm import tqdm
import matplotlib as mpl
import matplotlib.pyplot as plt
from datetime import date, datetime
from sklearn.cluster import KMeans
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error
from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder

# %matplotlib inline

np.random.seed(31415)

sns.set(rc={'figure.figsize':(15,3)})
pd.set_option('display.max_columns', None)

import warnings
warnings.filterwarnings('ignore')

from IPython.display import display, HTML
display(HTML("<style>.container { width:90% !important; }</style>"))

"""# Loading Data"""

i2m = list(zip(range(1,13), ['Gener','Febrer','Marc','Abril','Maig','Juny','Juliol','Agost','Setembre','Octubre','Novembre','Desembre']))
#for year in [2023, 2022, 2021, 2020]:
    #for month, month_name in i2m:
        #os.system(f"wget 'https://opendata-ajuntament.barcelona.cat/resources/bcn/BicingBCN/{year}_{month:02d}_{month_name}_BicingNou_ESTACIONS.7z'")
        #os.system(f"7z x '{year}_{month:02d}_{month_name}_BicingNou_ESTACIONS.7z'")
        #os.system(f"rm '{year}_{month:02d}_{month_name}_BicingNou_ESTACIONS.7z'")

# Mount Google Drive (only required if running from Google Colab)
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

df_2020_03 = pd.read_csv('/content/drive/MyDrive/Capstone_Project/data/2020_03_Marc_BicingNou_ESTACIONS.csv')

df_2020_03.head()

df_2020_03.shape

df_2020_03.info()

missing_val = df_2020_03.isna().sum()

print(missing_val)

"""## Add stations info"""

station_df = pd.read_csv('/content/drive/MyDrive/Capstone_Project/Informacio_Estacions_Bicing.csv')
station_df.head()

df_2020_03_info = pd.merge(df_2020_03, station_df, how='left') # TODO: Handle NaN station information

df_2020_03_info.head()

df_2020_03_info.isna().sum()

"""We don't have the info data for all the stations - Shere is working on this. I will drop for now all the rows with the capacity missing."""

df_2020_03_info_cleaned = df_2020_03_info.dropna(subset=['capacity'])

df_2020_03_info_cleaned.head(4)

df_2020_03_info_cleaned.isna().sum()

hh = df_2020_03_info_cleaned[df_2020_03_info_cleaned.num_docks_available > df_2020_03_info_cleaned.capacity]

hh.head()

hh.shape

"""There are some cases where num_docks_available is greater than capacity, which makes no sense."""

df_2020_03_info_cleaned.status.unique()

"""TODO: Select in the future just the values with "Status" -> "In Service" (then delete this variable)"""

def get_datetime(miliseconds: int):
  return datetime.fromtimestamp(miliseconds)

def get_var_date(df: pd.DataFrame):
  df['date'] = pd.to_datetime(df['last_reported'].apply(lambda x: get_datetime(x)))
  df['month'] = df['date'].dt.month
  df['year'] = df['date'].dt.year
  df['hour'] = df['date'].dt.hour
  df['day'] = df['date'].dt.day
  return df

def get_date(row):
  return datetime(int(row['year']), int(row['month']), int(row['day']), int(row['hour']))

def process_df(df: pd.DataFrame):
    # Drop duplicated rows
    df_2 = df.drop_duplicates()
    # Drop rows with NaN in the variable last_reported
    df_2 = df_2.dropna(subset=['last_reported'], axis=0)
    # Create date variables from timestamp
    df_2 = get_var_date(df_2)
    # Create the datetime variable
    df_2['date_time'] = df_2.apply(lambda row: get_date(row), axis=1)
    # Calculate the percentage of docks available
    df_2['percentage_docks_available'] = df_2['num_docks_available'] / df_2['capacity']
    # Sort the remaining data by last_reported
    df_2 = df_2.sort_values('last_reported', ascending=True)
    # Select only the necessary columns for now
    my_var = ['station_id', 'year', 'month', 'day', 'hour', 'percentage_docks_available']
    df_2 = df_2[my_var]
    # Group by station and date, and calculate the mean
    df_2 = df_2.groupby(['station_id', 'year', 'month', 'day', 'hour']).mean(numeric_only=True).reset_index()

    return df_2

new_df_2020_03_info = process_df(df_2020_03_info_cleaned)

new_df_2020_03_info.head()

new_df_2020_03_info.shape

new_df_2020_03_info = new_df_2020_03_info.sort_values(by=['station_id', 'year', 'month', 'day', 'hour']).reset_index(drop=True)

new_df_2020_03_info.head()

"""We can see some wrong years (1970 or 2019) generated using the timestamp. -> En la limpieza de datos inicial tener en cuenta esto para elminarlo. Ahora no lo elimino porque estoy viendo como hacer esto de las variables contexto y así no me paro."""

# De momento me quedo en este dataset solo lo que tenga año 2020 y mes 3, que es el ejemplo que he cogido:
new_df_2020_03_info = new_df_2020_03_info[(new_df_2020_03_info['year'] == 2020) & (new_df_2020_03_info['month'] == 3)].reset_index(drop=True)

new_df_2020_03_info.head()

# Función para agregar las columnas de contexto y eliminar filas usadas en cálculos desplazados
def add_context_columns(df):
    # DataFrame para almacenar los resultados
    df_ctx = pd.DataFrame()

    # Número de desplazamientos
    max_shift = 4

    # Iterar sobre cada station_id único
    for station_id in tqdm(df.station_id.unique()):
        # Filtrar el DataFrame por el station_id actual
        station_df = df[df.station_id == station_id].copy().reset_index(drop=True)

        # Iterar sobre el DataFrame a partir de max_shift para evitar data leakage
        for i in range(max_shift, len(station_df)):
            # Crear una copia de la fila actual
            current_row = station_df.iloc[[i]].copy()

            # Crear las columnas de contexto desplazadas
            for shift in range(1, max_shift + 1):
                current_row[f'ctx-{shift}'] = station_df.loc[i - shift, 'percentage_docks_available']

            # Añadir la fila actual con las columnas de contexto al DataFrame final
            df_ctx = pd.concat([df_ctx, current_row], axis=0)

    # Resetear el índice del DataFrame final
    df_ctx = df_ctx.reset_index(drop=True)

    return df_ctx

df_ctx = add_context_columns(new_df_2020_03_info)

df_ctx.head(40)



df_ctx = pd.DataFrame()

for station_id in tqdm(new_df_2020_03_info.station_id.unique()):
    station_df = new_df_2020_03_info[new_df_2020_03_info.station_id == station_id]
    station_df['ctx-4'] = station_df.percentage_docks_available.shift(4)
    station_df['ctx-3'] = station_df.percentage_docks_available.shift(3)
    station_df['ctx-2'] = station_df.percentage_docks_available.shift(2)
    station_df['ctx-1'] = station_df.percentage_docks_available.shift(1)

    # Eliminar filas con NaN en las columnas de contexto para evitar data leakage
    station_df = station_df.dropna(subset=['ctx-4', 'ctx-3', 'ctx-2', 'ctx-1'])

    df_ctx = pd.concat([station_df, df_ctx], axis=0).reset_index(drop=True)

df_ctx.head(40)

df_ctx.shape

df_ctx.isna().sum()